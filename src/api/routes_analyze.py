 # src/api/routes_analyze.py

from fastapi import APIRouter, HTTPException, Depends, status
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime
from sqlalchemy.orm import Session

# Importa o mÃ³dulo de integraÃ§Ã£o com os LLMs
from src.core.llm_integration import analyze_content_with_llm
# Importa as operaÃ§Ãµes CRUD para salvar o resultado da anÃ¡lise
from src.db import crud_operations
from src.db.database import get_db
from src.models.analysis import AnalysisCreate # Importa os schemas de Pydantic
from src.models.user import User # Importa o modelo de usuÃ¡rio
from src.core.users import current_active_user # Importa a dependÃªncia de usuÃ¡rio ativo

# Define o APIRouter com um prefixo e tags para organizar no Swagger UI
router = APIRouter(
    prefix="/analyze", # Este roteador serÃ¡ responsÃ¡vel pelo endpoint /analyze
    tags=["AI Analysis"] # Tag para agrupar no Swagger UI
)

# --- Schemas Pydantic para o Endpoint /analyze ---

class AnalyzeRequest(BaseModel):
    content: str = Field(..., description="O conteÃºdo textual a ser analisado.")
    # Permite escolher qual LLM usar, com Gemini como padrÃ£o
    preferred_llm: Optional[str] = Field("gemini", description="LLM preferencial para anÃ¡lise (gemini, openai ou huggingface).")

class AnalyzeResponse(BaseModel):
    id: str
    content: str
    classification: str
    status: str
    sources: Optional[List[str]] = None
    created_at: datetime
    updated_at: Optional[datetime] = None
    color: str = Field(..., description="Cor associada Ã  classificaÃ§Ã£o (ex: 'ðŸŸ¢', 'ðŸ”´', 'âšª', 'ðŸ”µ').")
    message: Optional[str] = Field(None, description="Mensagem ou justificativa da anÃ¡lise pelo LLM.")


# --- FunÃ§Ã£o Auxiliar para Mapear ClassificaÃ§Ã£o para Cor ---
def get_color_from_classification(classification: str) -> str:
    """
    Mapeia a classificaÃ§Ã£o do LLM para um emoji de cor.
    """
    lower_classification = classification.lower()
    if "fake_news" in lower_classification or "noticia_falsa" in lower_classification or "desinformacao" in lower_classification:
        return "ðŸ”´" # Vermelho para fake news
    elif "verdadeiro" in lower_classification or "fato" in lower_classification or "confirmado" in lower_classification:
        return "ðŸŸ¢" # Verde para verdadeiro
    elif "sÃ¡tira" in lower_classification or "humor" in lower_classification or "ficcao" in lower_classification:
        return "âšª" # Branco/Cinzento para sÃ¡tira
    elif "opiniÃ£o" in lower_classification or "editorial" in lower_classification or "perspectiva" in lower_classification:
        return "ðŸ”µ" # Azul para opiniÃ£o
    elif "parcial" in lower_classification or "tendencioso" in lower_classification:
        return "ðŸŸ " # Laranja para parcial
    else:
        return "âš«" # Preto para indefinido/erro

# --- Endpoint /analyze ---

@router.post("/", response_model=AnalyzeResponse, status_code=status.HTTP_201_CREATED)
async def analyze_content_endpoint(
    request_data: AnalyzeRequest,
    db: Session = Depends(get_db),
    user: User = Depends(current_active_user) # <-- AQUI: Adiciona a dependÃªncia de autenticaÃ§Ã£o
):
    """
    Analisa um conteÃºdo textual usando um modelo de linguagem (LLM)
    e armazena o resultado no banco de dados. Este endpoint requer autenticaÃ§Ã£o.
    """
    content = request_data.content
    preferred_llm = request_data.preferred_llm

    # 1. Chamar a LLM para anÃ¡lise
    print(f"INFO: Recebida solicitaÃ§Ã£o para analisar conteÃºdo com {preferred_llm}.")
    llm_result = {}
    try:
        llm_result = await analyze_content_with_llm(content, preferred_llm=preferred_llm)
        print(f"DEBUG: Resultado do LLM: {llm_result}")
    except Exception as e:
        print(f"ERRO: Falha ao chamar LLM: {e}")
        # Definir um resultado padrÃ£o em caso de falha da LLM
        llm_result = {"classification": "error", "message": f"Erro na anÃ¡lise da LLM: {e}"}

    classification = llm_result.get("classification", "indefinido")
    message = llm_result.get("message", "Nenhuma justificativa fornecida pela LLM ou erro na anÃ¡lise.")
    
    # 2. Mapear classificaÃ§Ã£o para cor
    color = get_color_from_classification(classification)

    # 3. Preparar dados para salvar no banco de dados
    analysis_create_data = AnalysisCreate(
        content=content,
        classification=classification,
        status="completed" if classification != "error" and classification != "indefinido" else "failed", # Marca como concluÃ­do
        sources=["LLM_analysis"], # Indica que a fonte Ã© a anÃ¡lise de LLM
    )

    # 4. Salvar a anÃ¡lise no banco de dados
    try:
        # VocÃª pode considerar adicionar o user.id aqui se quiser vincular a anÃ¡lise ao usuÃ¡rio
        db_analysis = crud_operations.create_analysis(db=db, analysis_data=analysis_create_data)
        print(f"DEBUG: AnÃ¡lise salva no BD com ID: {db_analysis.id}")
    except Exception as e:
        print(f"ERRO: Falha ao salvar anÃ¡lise no BD: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Falha ao salvar anÃ¡lise no banco de dados: {e}")

    # 5. Construir a resposta final
    response_data = AnalyzeResponse(
        id=str(db_analysis.id), # Garante que o ID seja uma string
        content=db_analysis.content,
        classification=db_analysis.classification,
        status=db_analysis.status,
        sources=db_analysis.sources,
        created_at=db_analysis.created_at,
        updated_at=db_analysis.updated_at,
        color=color, # Adiciona a cor
        message=message # Adiciona a mensagem da LLM
    )
    
    return response_data